{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Arrhythmia Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electrocardiograms (ECG) are records of the electrical activity of the heart gathered from electrodes placed on the skin. They are commonly used for medical monitoring and diagnosis.\n",
    "\n",
    "This ECG demo is based on the model developed by the [Stanford ML group](https://stanfordmlgroup.github.io/projects/ecg2/) using the [PhysioNet 2017 challenge dataset](https://www.physionet.org/content/challenge-2017/1.0.0/).  The [GitHub repository](https://github.com/awni/ecg) contains the original code and resources for training models.\n",
    "\n",
    "![Example ECG](figures/A00150.gif) \n",
    "\n",
    "Awni Y Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoffrey H Ti-son, Codie Bourn, Mintu P Turakhia, and Andrew Y Ng. Cardiologist-level arrhythmia  detection  and  classification  in  ambulatory electrocardiograms using a deep neural network. Nature Medicine, 25(1):65, 2019. https://www.nature.com/articles/s41591-018-0268-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the latest development version of Intel® Distribution of OpenVINO™ Toolkit. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit LTS version by running the cell below and following the link it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qarpo import displayMultiversionURL\n",
    "import os\n",
    "displayMultiversionURL(os.path.abspath(\"\"), \"Arrhythmia_Prediction.ipynb\", \"openvino-dev-latest\", ['openvino-lts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    - **Arrhythmia_prediction.ipynb** - This Jupyter* Notebook\n",
    "    - **figures/A00150.gif** - Example ECG shown at beginning of notebook\n",
    "    - **figures/EKG_info.svg** - Image showing the different parts of ECG rhythms\n",
    "    - **python/inference.py** - Python* code for inference jobs sent to edge nodes\n",
    "    - **python/keras_inference.py** - Python* code for inference done using Keras with a Tensorflow backend\n",
    "    - **python/load.py** - Python* code for loading files from the dataset\n",
    "    - **python/openvino_inference.py** - Python* code inference using OpenVINO\n",
    "    - **python/tensorflow_conversion.py** - Python* code for converting the Keras model into a tensorflow protobuf format\n",
    "    - **data/reference.csv** - Text file with class labels for the dataset\n",
    "    - **requirements.txt** - Python* requirements file\n",
    "    - **/data/ecg/0.427-0.863-020-0.290-0.899.hdf5** - Pre-trained model file\n",
    "    - **/data/ecg/training/*** - Location of the dataset files\n",
    "\n",
    "\n",
    "It is recommended that you have already read the following from [Get Started on the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/home/):\n",
    "- [Overview of the Intel® DevCloud for the Edge](https://devcloud.intel.com/edge/get_started/devcloud/)\n",
    "- [Overview of the Intel® Distribution of OpenVINO™ toolkit](https://devcloud.intel.com/edge/get_started/openvino/)\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this sample is being run on is on the Intel® DevCloud for the Edge which has Jupyter* Notebook customizations and all the required libraries already installed.  If you download or copy to a new server, this sample may not run.</i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements and Imports\n",
    "Running the inference requires Keras and serveral other packages to be installed. Run the following cells to ensure that all dependencies are satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Installing requirements\")\n",
    "!python3 -m pip --no-cache-dir install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from qarpo.demoutils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as ani\n",
    "import numpy as np\n",
    "import scipy.stats as sst\n",
    "import scipy.io as sio\n",
    "from IPython.display import HTML\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "This network is used to detect arrhythmias from ECG time series data. The model was trained using the PhysioNet Computing in Cardiology Challenge 2017 (CINC17) dataset, which has four distinct classes which are as follows:\n",
    "\n",
    "N - normal sinus rhythm\n",
    "\n",
    "A - atrial fibrillation (AF)\n",
    "\n",
    "O - an alternative rhythm\n",
    "\n",
    "~ - too noisy to be classified\n",
    "\n",
    "Although this dataset only has four distinct classifications for the ECG records, it is possible to train the model to distinguish between different types of arrhythmias if labels are provided to distinguish between different types.\n",
    "\n",
    "Both the Keras and the OpenVINO models will be run using a subset of the original test data that excludes any examples below a specified length. Although the Keras model can take inputs of variable size, we use the same input set as the OpenVINO examples for consistency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Information\n",
    "\n",
    "In this section we will see examples of the four different classes of arrhythmias and some of their distinguishing characteristics.\n",
    "\n",
    "The code below will convert the time series data into short animations which illustrate what each class looks like. Note that processing can take some time (~1-2 mins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ecg_animation(filename, y):\n",
    "    step_size = 12\n",
    "    x_lim = 1800\n",
    "    scale_factor = 7\n",
    "    num_frames = x_lim // step_size\n",
    "    x =  range(0,(x_lim*scale_factor),scale_factor)\n",
    "    y_max = 1.11*max(np.amax(y), abs(np.amin(y)))\n",
    "\n",
    "    plt.ioff()\n",
    "    \n",
    "    #set up the figure\n",
    "    fig, ax = plt.subplots()\n",
    "    line, = ax.plot([], [], color='k')\n",
    "    \n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0.0, labelbottom=False, labelleft=False)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Create the grid with 4x4 grid squares\n",
    "    aspect_ratio = 4200 / (2*y_max)\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(400))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(400 / aspect_ratio))\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(4))\n",
    "    ax.grid(which='major', linestyle='-', axis='both')\n",
    "    ax.grid(which='minor', linewidth='0.5', axis='both', color='lightgray')\n",
    "\n",
    "    # Settings for figure size\n",
    "    ax.set_xlim(1, x_lim*scale_factor)\n",
    "    ax.set_ylim(-y_max, y_max)\n",
    "    fig.set_figheight(4.5)\n",
    "    fig.set_figwidth(13.5)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    canvas_width, canvas_height = fig.canvas.get_width_height()\n",
    "\n",
    "    def update(num):\n",
    "        offset = int(num // (x_lim / step_size)) + 2\n",
    "        index = int(num % (x_lim / step_size))\n",
    "        line.set_data(x[:(step_size*index)], y[(x_lim*offset):(x_lim*offset+step_size*index)])\n",
    "\n",
    "    # Open an ffmpeg process\n",
    "    cmdstring = ('ffmpeg', \n",
    "                 '-y', '-r', '25', # 25fps\n",
    "                 '-s', '%dx%d' % (canvas_width, canvas_height), # size of image string\n",
    "                 '-pix_fmt', 'argb', # format\n",
    "                 '-f', 'rawvideo',  '-i', '-', # tell ffmpeg to expect raw video from the pipe\n",
    "                 '-preset', 'ultrafast',\n",
    "                 '-vcodec', 'h264', 'figures/' + filename) # output encoding\n",
    "    p = subprocess.Popen(cmdstring, stdin=subprocess.PIPE)\n",
    "\n",
    "    # Draw frames and write to the pipe\n",
    "    for frame in range(num_frames):\n",
    "        # draw the frame\n",
    "        update(frame)\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        # extract the image as an ARGB string\n",
    "        string = fig.canvas.tostring_argb()\n",
    "\n",
    "        # write to pipe\n",
    "        p.stdin.write(string)\n",
    "\n",
    "    # Finish up\n",
    "    p.communicate()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Sinus Rhythm\n",
    "\n",
    "Normal ECG rhythms consist of four distinct sections: P wave, QRS complex, T wave, and U wave.\n",
    "\n",
    "<figure>\n",
    "<img src=\"figures/EKG_info.svg\" height=40%, width=40%/>\n",
    "<figcaption style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:EKG_Complex_en.svg\" title=\"via Wikimedia Commons\">ECG Complex</a> [<a href=\"https://creativecommons.org/licenses/by-sa/3.0\">CC BY-SA</a>]</figcaption>\n",
    "</figure>\n",
    "\n",
    "The P wave represents atrial depolarization.  \n",
    "The QRS complex represents ventricular depolarization.  \n",
    "The T wave represents ventricular repolarization.  \n",
    "The U wave represents papillary muscle repolarization.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_normal = sio.loadmat('/data/ecg/training/A00001.mat')['val'].squeeze()\n",
    "\n",
    "create_ecg_animation('ecg_normal.mp4', ecg_normal)\n",
    "\n",
    "HTML('''\n",
    "    <video alt=\"test\" controls autoplay loop>\n",
    "        <source src=\"figures/ecg_normal.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atrial Fibrilation\n",
    "\n",
    "Atrial fibrilation is usually distinguished by irregular intervals between heart beats, rapid heart rate, and lack of a P wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_af = sio.loadmat('/data/ecg/training/A00004.mat')['val'].squeeze()\n",
    "\n",
    "create_ecg_animation(\"ecg_af.mp4\", ecg_af)\n",
    "\n",
    "HTML('''\n",
    "    <video alt=\"test\" controls autoplay loop>\n",
    "        <source src=\"figures/ecg_af.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Rhythm\n",
    "\n",
    "For this dataset, all non-AF abnormal rhythms are classified as other rhythms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_other = sio.loadmat('/data/ecg/training/A00077.mat')['val'].squeeze()\n",
    "\n",
    "create_ecg_animation(\"ecg_other.mp4\", ecg_other)\n",
    "\n",
    "HTML('''\n",
    "    <video alt=\"test\" controls autoplay loop>\n",
    "        <source src=\"figures/ecg_other.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too noisy to be classified\n",
    "\n",
    "This final classification includes data that has too much noise to have any distinguishable pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_undef = sio.loadmat('/data/ecg/training/A01246.mat')['val'].squeeze()\n",
    "\n",
    "create_ecg_animation(\"ecg_undef.mp4\", ecg_undef)\n",
    "\n",
    "HTML('''\n",
    "    <video alt=\"test\" controls autoplay loop>\n",
    "        <source src=\"figures/ecg_undef.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Prediction Using Keras\n",
    "\n",
    "In this section we will run all of the sample data through Keras using a Tensorflow backend. After getting predictions from the model, we will compare it to the ground truth labels to measure accuracy. The inference it done by running the [keras_inference.py](./python/keras_inference.py) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 python/keras_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Keras Model to an OpenVINO Intermediate Representation\n",
    "\n",
    "Next we want to run the model through the OpenVINO model optimizer to produce a Intermediate Representation (IR) that can be used to run inference using the OpenVINO inference engine. However, in order to make that possible we first have to convert the model to a format that is supported by the model optimizer. \n",
    "\n",
    "`hdf5` Keras → `pb` Tensorflow → `IR` OpenVINO\n",
    "\n",
    " Running the following cell takes the original keras model file `.hdf5` and converts it to a tensorflow frozen protobuf format `.pb` and will print out a summary of the original model for reference. The script for this conversion process can be found at [tensorflow_conversion.py](./python/tensorflow_conversion.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 python/tensorflow_conversion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `.pb` model file, we can pass that to the model optimizer for conversion. Running the model optimizer will generate a `.xml` and `.bin` which represent the network topology and model weights. \n",
    "\n",
    "We need to specify the size of the input for the model optimizer and we use `[1,8960,1]` to match our sample length. We also specify the data type as FP16 to support inference on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo_tf.py \\\n",
    "--input_model models/output_graph.pb                                   \\\n",
    "--output_dir models/                                                   \\\n",
    "--input_shape \"[1,8960,1]\"                                             \\\n",
    "--data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference Using OpenVINO\n",
    "\n",
    "The OpenVINO model only takes input of a specific size so we truncate all of the data that is above that size before feeding it into the model. The script for running the OpenVINO inference can be found at [openvino_inference.py](./python/openvino_inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 python/openvino_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results\n",
    "\n",
    "The following cell creates a visualization to show the results of the network's predictions. Different colors are used to represent the predicted class (N - blue, A - red, O - yellow, ~ - gray) with a score for the confidence in the prediction at the bottom of each section. The final predicted class is determined by taking the most commonly predicted class across all of the time slices for the sample. Only the first 20 time slices are shown below to allow for greater visual clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(sample, prediction, probabilities, actual):\n",
    "    data_points = 256*20\n",
    "    \n",
    "    predicted = sst.mode(prediction)[0][0]\n",
    "    y = sio.loadmat('/data/ecg/training/' + sample)['val'].squeeze()[:data_points]\n",
    "    \n",
    "    x =  range(0,data_points)\n",
    "    y_max = 1.11*max(np.amax(y), abs(np.amin(y)))\n",
    "    \n",
    "    #set up the figure\n",
    "    fig, ax = plt.subplots()\n",
    "    line, = ax.plot(x, y, color='k')\n",
    "    ax.title.set_text(sample + ', Predicted Class - ' + predicted + ', Actual Class - ' + actual)\n",
    "    \n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0.0, labelbottom=False, labelleft=False)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Add axis lines\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(256))\n",
    "    ax.grid(which='major', linestyle='-', axis='x', color='k')\n",
    "\n",
    "    # Settings for figure size\n",
    "    ax.set_xlim(1, data_points)\n",
    "    ax.set_ylim(-y_max, y_max)\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(12)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    i = 0\n",
    "    for label, prob in zip(prediction[:20], probabilities[:20]):\n",
    "        colors = {'A' : 'r', 'N' : 'b', 'O' : 'y', '~': 'k'}\n",
    "        plt.text((i+.2)*256, -y_max, '%.3f' % prob)\n",
    "        plt.axvspan(i*256, (i+1)*256, facecolor=colors[label], alpha=prob/3)\n",
    "        i += 1\n",
    "        \n",
    "    plt.show(fig)    \n",
    "    plt.close(fig)\n",
    "\n",
    "def show_all_predictions():\n",
    "    with open('results/predictions.json', 'r') as preds:\n",
    "        preds = json.load(preds)\n",
    "        items = list(preds.items())\n",
    "        list(map(lambda x: visualize_result(x[0], x[1]['data'], x[1]['prob'], x[1]['actual']), items))\n",
    "\n",
    "def show_prediction_subset(items):\n",
    "    with open('results/predictions.json', 'r') as preds:\n",
    "        preds = json.load(preds)\n",
    "        list(map(lambda x: visualize_result(x, preds[x]['data'], preds[x]['prob'], preds[x]['actual']), items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions with accurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prediction_subset(['A02261', 'A06779', 'A00593', 'A02479'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions with lower confidence or inaccurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prediction_subset(['A00722', 'A04222', 'A08010', 'A08525'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) View all of the evaluation data\n",
    "\n",
    "Run the follwing cell to display all 300 of the samples and the predictions given by the model. Due to the number of samples, it may take a couple minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the edge\n",
    "\n",
    "All the code up to this point has been run on a development node based on an Intel Xeon Scalable processor. We will run the workload on other edge compute nodes represented in the IoT DevCloud by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node. For this example, we have written the job file for you in the notebook. It performs the classification using the script [inference.py](./python/inference.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prediction.sh\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "mkdir -p $1\n",
    "OUTPUT_DIR=$1\n",
    "DEVICE=$2\n",
    "\n",
    "python3 python/inference.py -d ${DEVICE} -o ${OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit the job to 6 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are five options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets us name the job so that it is easier to distinguish between them.\n",
    "- `-o` : this option lets us determine the path to be used for the standard output stream.\n",
    "- `-e` : this option lets us determine the path to be used for the standard error stream.\n",
    "\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [prediction.sh](prediction.sh) script takes in 2 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU, GPU, MYRIAD, HDDL)\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>You can submit all the jobs at once or follow one at a time.</i></div> \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>**Shift+Enter** will run the cell and automatically move you to the next cell. This allows you to use **Shift+Enter** multiple times to quickly run through multiple cells, including markdown cells.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_core = !qsub prediction.sh -l nodes=1:idc001skl -F \"results/core/ CPU\" -N arrhythmia_core -e results/core/ -o results/core/   \n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('./logs', job_id_core[0] + '_load.txt', \"Data Loading\", 0, 100)\n",
    "    progressIndicator('./logs', job_id_core[0]+'.txt', \"Inference\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an 8th Generation Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/8th-gen-core-dev-kit\">UP Xtreme Edge Compute Enabling Kit\n",
    "    </a> edge node with a low power <a \n",
    "    href=\"https://ark.intel.com/content/www/us/en/ark/products/193554/intel-core-i7-8665ue-processor-8m-cache-up-to-4-40-ghz.html\">Intel \n",
    "    Core i7-8865UE</a>. The inference workload will run on the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_core2 = !qsub prediction.sh -l nodes=1:idc014upxa10fx1 -F \"results/core2/ CPU\" -N arrhythmia_core2 -e results/core2/ -o results/core2/\n",
    "print(job_id_core2[0]) \n",
    "#Progress indicators\n",
    "if job_id_core2:\n",
    "    progressIndicator('./logs', job_id_core2[0] + '_load.txt', \"Data Loading\", 0, 100)\n",
    "    progressIndicator('./logs', job_id_core2[0]+'.txt', \"Inference\", 0, 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit to an edge compute node with Intel® Xeon® Gold 6258R CPU\n",
    "In the cell below, we submit a job to an edge node with an [Intel® Xeon® Gold 6258R Processor](https://ark.intel.com/content/www/us/en/ark/products/199350/intel-xeon-gold-6258r-processor-38-5m-cache-2-70-ghz.html). The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_xeon_cascade_lake = !qsub prediction.sh -l nodes=1:idc018 -F \"results/xeon_cascade_lake/ CPU\" -N arrhythmia_xeon_cascade_lake -e results/xeon_cascade_lake/ -o results/xeon_cascade_lake/\n",
    "print(job_id_xeon_cascade_lake[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon_cascade_lake:\n",
    "    progressIndicator('./logs', job_id_xeon_cascade_lake[0] + '_load.txt', \"Data Loading\", 0, 100)\n",
    "    progressIndicator('./logs', job_id_xeon_cascade_lake[0]+'.txt', \"Inference\", 0, 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Xeon® E3-1268L v5 CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_xeon_skylake = !qsub prediction.sh -l nodes=1:idc007xv5 -F \"results/xeon_skylake/ CPU\" -N arrhythmia_xeon_skylake -e results/xeon_skylake/ -o results/xeon_skylake/\n",
    "print(job_id_xeon_skylake[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon_skylake:\n",
    "    progressIndicator('./logs', job_id_xeon_skylake[0] + '_load.txt', \"Data Loading\", 0, 100)\n",
    "    progressIndicator('./logs', job_id_xeon_skylake[0]+'.txt', \"Inference\", 0, 100)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_gpu = !qsub prediction.sh -l nodes=1:idc001skl -F \"results/gpu/ GPU\" -N arrhythmia_gpu -e results/gpu/ -o results/gpu/\n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('./logs', job_id_gpu[0] + '_load.txt', \"Data Loading\", 0, 100)\n",
    "    progressIndicator('./logs', job_id_gpu[0]+'.txt', \"Inference\", 0, 100)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with UP Squared Grove IoT Development Kit (UP2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel® Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_up2 = !qsub prediction.sh -l nodes=1:idc008u2g -F \"results/up2/ GPU\" -N arrhythmia_up2 -e results/up2/ -o results/up2/\n",
    "print(job_id_up2[0]) \n",
    "#Progress indicators\n",
    "if job_id_up2:\n",
    "    progressIndicator('./logs', job_id_up2[0] + '_load.txt', \"Data Loading\", 0, 100)\n",
    "    progressIndicator('./logs', job_id_up2[0]+'.txt', \"Inference\", 0, 100)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "<br><div class=note><i><b>\n",
    "Note: The amount of time spent in the queue depends on the number of users accessing the requested compute nodes. Once the jobs for this sample application begin to run, they should take from 10 to 40 seconds each to complete.\n",
    "</b></i></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><div class=danger><b>Wait!: </b>Please wait for the inference jobs and video rendering to complete before proceeding to the next step to view results.</div>\n",
    "\n",
    "## Compare Results\n",
    "\n",
    "The running time of each inference task is recorded in `results/{arch}/stats_{job_id}.txt` and the output of the inference is stored at `results/{arch}/{job_name}.o{job_id}`. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('core2', 'Intel Core\\ni7-8865UE\\nCPU'),\n",
    "             ('xeon_cascade_lake', 'Intel Xeon\\nGold\\n 6258R\\nCPU'),\n",
    "             ('xeon_skylake', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/' + arch + '/stats_'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "plt.ion()\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, miliseconds', 'Inference Engine Processing Time Per Sample', 'time' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemetry Dashboard\n",
    "Once your submitted jobs are completed, run the cells below to view telemetry dashboards containing performance metrics for your model and target architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Core™ i5-6500TE</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_core[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Core™ i7-8865UE</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_core2[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Xeon® Gold 6258R CPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_xeon_cascade_lake[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Xeon® E3-1268L v5 CPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_xeon_skylake[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on Intel® Core CPU and using the onboard Intel® GPU</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_gpu[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_t = \"<a target='_blank' href='{href}'> Click here to view telemetry dashboard of the last job ran on UP Squared Grove IoT Development Kit (UP2)</a>\"\n",
    "\n",
    "result_file = \"https://devcloud.intel.com/edge/metrics/d/\" + job_id_up2[0].split('.')[0]\n",
    "\n",
    "html = HTML(link_t.format(href=result_file))\n",
    "\n",
    "display(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.1)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
