{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Object Detection Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the latest development version of Intel® Distribution of OpenVINO™ Toolkit. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit LTS version by running the cell below and following the link it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "from qarpo import displayMultiversionURL\n",
    "import os\n",
    "displayMultiversionURL(os.path.abspath(\"\"), \"tutorial_object_detection_ssd.ipynb\", \"openvino-dev-latest\", ['openvino-lts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "This tutorial requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    - **tutorial_object_detection_ssd.ipynb** - This Jupyter Notebook\n",
    "    - **mobilenet-ssd/mobilenet-ssd.bin and mobilenet-ssd.xml** - The IR files for the inference model created using Model Optimizer\n",
    "    - **mobilenet-ssd/labels.txt** - Mapping of numerical labels to text strings\n",
    "    - **face.jpg** - Test image\n",
    "    - **car.bmp** - Test image\n",
    "    - **doc_*.png** - Images used in the documentation\n",
    "- Optional: URL to user's image or video to run inference on\n",
    "\n",
    "<br><div class=note><i><b>Note: </b>It is assumed that the server this tutorial is being run on has Jupyter* Notebook, the Intel® Distribution of OpenVINO™ toolkit, and other required libraries already installed.  If you download or copy to a new server, this tutorial may not run.</i></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this tutorial is to examine a sample application that was created using the [Intel® Distribution of Open VINO™ toolkit (Open Visual Inference & Neural Network Optimization)](https://software.intel.com/openvino-toolkit).  This tutorial will go step-by-step through the necessary steps to demonstrate object detection on images and video.  Object detection is performed using a pre-trained network and running it using the Intel® Distribution of OpenVINO™ toolkit Inference Engine.  Inference will be executed using the same CPU(s) running this Jupyter* Notebook.\n",
    "\n",
    "The pre-trained model to be used for object detection is the [\"mobilenet-ssd\"](https://github.com/chuanqi305/MobileNet-SSD) which has already been converted to the necessary Intermediate Representation (IR) files needed by the Inference Engine (Conversion is not covered here, please see the [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/en-us/openvino-toolkit) documentation for more details).  The model is capable of detecting \n",
    "different objects including: airplane, bicycle, bird, boat, bus, car, cat, dog, horse, person and more (see mobilenet-ssd/labels.txt file for complete list).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key concepts\n",
    "\n",
    "Before going into the samples in the tutorial steps, first we will go over some key concepts that will be covered in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intel® distribution of OpenVINO™ toolkit overview and terminology\n",
    "\n",
    "Let us begin with a brief overview of the Intel® Distribution of OpenVINO™ toolkit and what this tutorial will be covering.  The Intel® Distribution of OpenVINO™ toolkit enables the quick deployment of convolutional neural networks (CNN) for heterogeneous execution on Intel® hardware while maximizing performance. This is done using the Intel® Deep Learning Deployment Toolkit (Intel® DLDT) included within the Intel® Distribution of OpenVINO™ toolkit with its main components shown below.\n",
    "\n",
    "![image alt text](./doc_openvino_overview_image.png)\n",
    "\n",
    "The basic flow is:\n",
    "\n",
    "1. Use a tool, such as Caffe*, to create and train a CNN inference model\n",
    "\n",
    "2. Run the created model through Model Optimizer to produce an optimized Intermediate Representation (IR) stored in files (`.bin` and `.xml`) for use with the Inference Engine\n",
    "\n",
    "3. The User Application then loads and runs models on devices using the Inference Engine and the IR files  \n",
    "\n",
    "This tutorial will focus on the last step, the User Application and using the Inference Engine to run a model on a CPU.\n",
    "\n",
    "##### Using the inference engine\n",
    "\n",
    "Below is a more detailed view of the User Application and Inference Engine:\n",
    "\n",
    "![image alt text](./doc_inference_engine_image.png)\n",
    "\n",
    "The Inference Engine includes a plugin library for each supported device that has been optimized for the Intel® hardware device CPU, GPU, and VPU.  From here, we will use the terms \"device\" and “plugin” with the assumption that one infers the other (e.g. CPU device infers the CPU plugin and vice versa).  As part of loading the model, the User Application tells the Inference Engine which device to target which in turn loads the associated plugin library to later run on the associated device. The Inference Engine uses “blobs” for all data exchanges, basically arrays in memory arranged according the input and output data of the model.\n",
    "\n",
    "###### Inference engine API integration flow\n",
    "\n",
    "Using the inference engine API follows the basic steps outlined briefly below.  The API objects and functions will be seen later in the sample code.\n",
    "\n",
    "1. Load the plugin\n",
    "\n",
    "2. Read the model IR\n",
    "\n",
    "3. Load the model into the plugin\n",
    "\n",
    "6. Prepare the input\n",
    "\n",
    "7. Run inference\n",
    "\n",
    "8. Process the output\n",
    "\n",
    "More details on the Inference Engine can be found in the [Inference Engine Development Guide](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input preprocessing\n",
    "\n",
    "Often, the dimensions of the input data does not match the required dimensions of the input data for the inference model.  A common example is an input video frame.  Before the image may be input to the inference model, the input must be preprocessed to match the required dimensions for the inference model as well as channels (i.e. colors) and batch size (number of images present).  The basic step performed is to resize the frame from the source dimensions to match the required dimensions of the inference model’s input, reorganizing any dimensions as needed.\n",
    "\n",
    "This tutorial and the many samples in the Intel® Distribution of OpenVINO™ toolkit use OpenCV to perform resizing of input data.  The basic steps performed using OpenCV are:\n",
    "\n",
    "1.  Resize image dimensions form image to model's input W x H:<br>\n",
    "`    frame = cv2.resize(image, (w, h))`\n",
    "   \n",
    "2. Change data layout from (H x W x C) to (C x H x W)<br>\n",
    "`    frame = frame.transpose((2, 0, 1))`  \n",
    "\n",
    "3. Reshape to match input dimensions<br>\n",
    "`    frame = frame.reshape((n, c, h, w))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample application\n",
    "\n",
    "The following sections will guide you through a sample application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "We begin by importing all of the Python* modules that will be used by the sample code:\n",
    "- [os](https://docs.python.org/3/library/os.html#module-os) - Operating system specific module (used for file name parsing)\n",
    "- [cv2](https://docs.opencv.org/trunk/) - OpenCV module\n",
    "- [time](https://docs.python.org/3/library/time.html#module-time) - time tracking module (used for measuring execution time)\n",
    "- [openvino.inference_engine](https://software.intel.com/en-us/articles/OpenVINO-InferEngine) - the IENetwork and IECore objects\n",
    "- [matplotlib](https://matplotlib.org/) - pyplot is used for displaying output images\n",
    "\n",
    "Run the cell below to import Python dependencies needed for displaying the results in this notebook. \n",
    "\n",
    "<br><div class=tip><b>Tip: </b>Select a cell and then use **Ctrl+Enter** to run that cell.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from openvino.inference_engine import IECore\n",
    "import ngraph as ng\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "print('Imported Python modules successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the IR files for the inference model\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) used to convert and optimize trained models into the Intermediate Representation (IR) model files, and the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) that uses the IR model files to run inference on hardware devices.  The IR model files can be created from trained models from popular frameworks (e.g. Caffe\\*, Tensorflow*, etc.). \n",
    "The Intel® Distribution of OpenVINO™ toolkit also includes the [Model Downloader](http://docs.openvinotoolkit.org/latest/_tools_downloader_README.html) utility  to download some common inference models from the [Open Model Zoo](https://github.com/opencv/open_model_zoo). \n",
    "\n",
    "Run the following cell to run the Model Downloader utility with the `--print_all` argument to see all the available inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!downloader.py --name mobilenet-ssd -o raw_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use converter.py to converts the models that are not in the Inference Engine IR format into that format using Model Optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!converter.py --name mobilenet-ssd -d raw_model -o model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "Here we will create and set the following configuration parameters used by the sample:  \n",
    "* **model_xml** - Path to the .xml IR file of the trained model to use for inference\n",
    "* **model_bin** - Path to the .bin IR file of the trained model to use for inference (derived from *model_xml*)\n",
    "* **input_path** - Path to input image\n",
    "* **device** - Specify the target device to infer on,  CPU, GPU, or MYRIAD is acceptable, however the device must be present.  For this tutorial we use \"CPU\" which is known to be present.\n",
    "* **labels_path** - Path to labels mapping file used to map outputted integers to strings (e.g. 7=\"car\")\n",
    "* **prob_threshold** - Probability threshold for filtering detection results\n",
    "\n",
    "We will set all parameters here only once except for `input_path` which we will change later to point to different images and video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model IR files\n",
    "model_xml = \"model/public/mobilenet-ssd/FP32/mobilenet-ssd.xml\"\n",
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\" # create IR .bin filename from path to IR .xml file\n",
    "\n",
    "# input image file\n",
    "input_path = \"car.bmp\"\n",
    "\n",
    "# CPU extension library to use\n",
    "cpu_extension_path = os.path.expanduser(\"~\")+\"/inference_engine_samples/intel64/Release/lib/libcpu_extension.so\"\n",
    "\n",
    "# device to use\n",
    "device = \"CPU\"\n",
    "\n",
    "# output labels \n",
    "labels_path = \"labels.txt\"\n",
    "\n",
    "# minimum probability threshold to detect an object\n",
    "prob_threshold = 0.5\n",
    "\n",
    "print(\"Configuration parameters settings:\"\n",
    "     \"\\n\\tmodel_xml=\", model_xml,\n",
    "      \"\\n\\tmodel_bin=\", model_bin,\n",
    "      \"\\n\\tinput_path=\", input_path,\n",
    "      \"\\n\\tdevice=\", device, \n",
    "      \"\\n\\tlabels_path=\", labels_path, \n",
    "      \"\\n\\tprob_threshold=\", prob_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference engine instance\n",
    "\n",
    "Next we create the Inference Engine instance to be used by our application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Inference Engine instance\n",
    "ie = IECore()\n",
    "print(\"An Inference Engine object has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create network\n",
    "\n",
    "Here we create an IENetwork object and load the model's IR files into it. After loading the model, we check to make sure that all the model's layers are supported by the plugin we will use. We also check to make sure that the model's input and output are as expected for later when we run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network from IR files\n",
    "net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "print(\"Loaded model IR files [\",model_bin,\"] and [\", model_xml, \"]\\n\")\n",
    "\n",
    "# check to make sure that the plugin has support for all layers in the loaded model\n",
    "supported_layers = ie.query_network(net, \"CPU\") \n",
    "ng_function = ng.function_from_cnn(net)\n",
    "not_supported_layers = \\\n",
    "[node.get_friendly_name() for node in ng_function.get_ordered_ops() if node.get_friendly_name() not in supported_layers]\n",
    "if len(not_supported_layers) != 0:\n",
    "    log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(device, ', '.join(not_supported_layers)))\n",
    "    log.error(\"Please try to specify cpu extensions library path in sample's command line parameters using\"\n",
    "                      \"--cpu_extension command line argument\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# check to make sue that the model's input and output are what is expected\n",
    "assert len(net.input_info.keys()) == 1, \\\n",
    "    \"ERROR: This sample supports only single input topologies\"\n",
    "assert len(net.outputs) == 1, \\\n",
    "    \"ERROR: This sample supports only single output topologies\"\n",
    "print(\"SUCCESS: Model IR files have been loaded and verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model\n",
    "\n",
    "Here we load the model network into the plugin so that we may run inference.  `exec_net` will be used later to actually run inference.  After loading, we store the names of the input (`input_blob`) and output (`output_blob`) blobs to use when accessing the input and output blobs of the model.  Lastly, we store the model's input dimensions into the following variables:\n",
    "- `n` = input batch size\n",
    "- `c` = number of input channels (here 1 channel per color R,G, and B)\n",
    "- `h` = input height\n",
    "- `w` = input width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model into the Inference Engine for our device\n",
    "exec_net = ie.load_network(network=net, num_requests=2, device_name=device)\n",
    "\n",
    "# store name of input and output blobs\n",
    "input_blob = next(iter(net.input_info))\n",
    "output_blob = next(iter(net.outputs))\n",
    "\n",
    "# read the input's dimensions: n=batch size, c=number of channels, h=height, w=width\n",
    "n, c, h, w = net.input_info[input_blob].input_data.shape\n",
    "print(\"Loaded model into Inference Engine for device:\", device, \n",
    "      \"\\nModel input dimensions: n=\",n,\", c=\",c,\", h=\",h,\", w=\",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labels\n",
    "\n",
    "For each detected object, the output from the model will include an integer to indicate which type (e.g. car, person, etc.) of trained object has been detected.  To translate the integer into a more readable text string, a label mapping file may be used.  The label mapping file is simply a text file of the format \"n: string\" (e.g. \"7: car\" for 7=\"car\") that is loaded into a lookup table to be used later while labeling detected objects.\n",
    "\n",
    "Here, if the `labels_path` variable has been set to point to a label mapping file, we open the file and load the labels into the variable `labels_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = None\n",
    "# if labels points to a label mapping file, then load the file into labels_map\n",
    "print(labels_path)\n",
    "if os.path.isfile(labels_path):\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels_map = [x.split(sep=' ', maxsplit=1)[-1].strip() for x in f]\n",
    "    print(\"Loaded label mapping file [\",labels_path,\"]\")\n",
    "else:\n",
    "    print(\"No label mapping file has been loaded, only numbers will be used\",\n",
    "          \" for detected object labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input\n",
    "\n",
    "Here we read and then prepare the input image by resizing and re-arranging its dimensions according to the model's input dimensions. We define the functions `loadInputImage()` and `resizeInputImage()` for the operations so that we may reuse them again later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to load an input image\n",
    "def loadInputImage(input_path, verbose = True):\n",
    "    # globals to store input width and height\n",
    "    global input_w, input_h\n",
    "    \n",
    "    # use OpenCV to load the input image\n",
    "    cap = cv2.VideoCapture(input_path) \n",
    "    \n",
    "    # store input width and height\n",
    "    input_w = cap.get(3)\n",
    "    input_h = cap.get(4)\n",
    "    if verbose: print(\"Loaded input image [\",input_path,\"], resolution=\", input_w, \"w x \",input_h,\"h\")\n",
    "\n",
    "    # load the input image\n",
    "    ret, image = cap.read()\n",
    "    del cap\n",
    "    return image\n",
    "\n",
    "# define function for resizing input image\n",
    "def resizeInputImage(image, verbose = True):\n",
    "    # resize image dimensions form image to model's input w x h\n",
    "    in_frame = cv2.resize(image, (w, h))\n",
    "    # Change data layout from HWC to CHW\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  \n",
    "    # reshape to input dimensions\n",
    "    in_frame = in_frame.reshape((n, c, h, w))\n",
    "    if verbose: print(\"Resized input image from {} to {}\".format(image.shape[:-1], (h, w)))\n",
    "    return in_frame\n",
    "\n",
    "# load image\n",
    "image = loadInputImage(input_path)\n",
    "\n",
    "# resize the input image\n",
    "in_frame = resizeInputImage(image)\n",
    "\n",
    "# display input image\n",
    "print(\"Input image:\")\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "\n",
    "Now that we have the input image in the correct format for the model, we now run inference on the input image that was previously set to `./car.bmp`:\n",
    "<img src=\"car.bmp\" style=\"width:150px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save start time\n",
    "inf_start = time.time()\n",
    "\n",
    "# run inference\n",
    "res = exec_net.infer(inputs={input_blob: in_frame})   \n",
    "\n",
    "# calculate time from start until now\n",
    "inf_time = time.time() - inf_start\n",
    "print(\"Inference complete, run time: {:.3f} ms\".format(inf_time * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process results\n",
    "Now we parse the inference results and for each object detected draw boxes with text annotations on image.  We define the function `processResults()` so that we may use it again later in the tutorial to process results.\n",
    "\n",
    "`res` is set to the output of the inference model which is an array of results, with one element for each detected object.  We loop through `res` setting `obj` to hold the results for each detected object which appear in `obj` as:\n",
    "- `obj[1]` = Class ID (type of object detected)\n",
    "- `obj[2]` = Probability of detected object\n",
    "- `obj[3]` = Lower x coordinate of detected object \n",
    "- `obj[4]` = Lower y coordinate of detected object\n",
    "- `obj[5]` = Upper x coordinate of detected object\n",
    "- `obj[6]` = Upper y coordinate of detected object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to process inference results\n",
    "def processResults(result):\n",
    "    # get output results\n",
    "    res = result[output_blob]\n",
    "    \n",
    "    # loop through all possible results\n",
    "    for obj in res[0][0]:\n",
    "        # If probability is more than specified threshold, draw and label box \n",
    "        if obj[2] > prob_threshold:\n",
    "            # get coordinates of box containing detected object\n",
    "            xmin = int(obj[3] * input_w)\n",
    "            ymin = int(obj[4] * input_h)\n",
    "            xmax = int(obj[5] * input_w)\n",
    "            ymax = int(obj[6] * input_h)\n",
    "            \n",
    "            # get type of object detected\n",
    "            class_id = int(obj[1])\n",
    "            \n",
    "            # Draw box and label for detected object\n",
    "            color = (min(class_id * 12.5, 255), 255, 255)\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 4)\n",
    "            det_label = labels_map[class_id] if labels_map else str(class_id)\n",
    "            cv2.putText(image, det_label + ' ' + str(round(obj[2] * 100, 1)) + ' %', (xmin, ymin - 7),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 1, color, 2)\n",
    "\n",
    "processResults(res)\n",
    "print(\"Processed inference output results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results\n",
    "Now that the results from inference have been processed, we display the image to see what has been detected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert colors BGR -> RGB\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# disable axis display, then display image\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #1: Run a different image\n",
    "Now that we have seen all the steps, let us run them again on a different image.  We also define *inferImage()* to combine the input processing, inference, and results processing so that we may use it again later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to prepare input, run inference, and process inference results\n",
    "def inferImage(image, verbose = True):\n",
    "    # prepare input\n",
    "    in_frame = resizeInputImage(image, verbose)\n",
    "\n",
    "    # run inference\n",
    "    res = exec_net.infer(inputs={input_blob: in_frame})   \n",
    "\n",
    "    # process inference results \n",
    "    processResults(res)\n",
    "\n",
    "# set path to different input image\n",
    "input_path=\"face.jpg\"\n",
    "\n",
    "# load input image\n",
    "image = loadInputImage(input_path)\n",
    "\n",
    "# display input image\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# infer image\n",
    "inferImage(image)\n",
    "\n",
    "# display image with inference results\n",
    "# convert colors BGR -> RGB\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# create new figure, disable axis display, then display image\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #2: (Optional) Run your own image\n",
    "\n",
    "Here you may run any image you would like by setting the `input_path` variable which may be set to a local file or URL.  A sample URL is provided as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_path may be set to a local file or URL\n",
    "input_path=\"https://github.com/chuanqi305/MobileNet-SSD/raw/master/images/004545.jpg\"\n",
    "\n",
    "# load input image\n",
    "image = loadInputImage(input_path)\n",
    "\n",
    "# display input image\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "# infer image\n",
    "inferImage(image)\n",
    "\n",
    "# display image with inference results\n",
    "# convert colors BGR -> RGB\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# create figure, disable axis display, then display image\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #3: Running inference on video\n",
    "We have seen how to run individual images, now how do we do video?  To run inference on video is much the same as for a single image except that a loop is necessary to process all the frames in the video.  In the code below, we use the same method of loading a video as we had for an image, but now include the while-loop to keep reading images until `cap.isOpened()` returns false or `cap.read()` sets `ret` to false:\n",
    "```python\n",
    "while cap.isOpened():\n",
    "    # read video frame\n",
    "    ret, im = cap.read()\n",
    "   \n",
    "    # break if no more video frames\n",
    "    if not ret:\n",
    "        break  \n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close and then re-import matplotlib to be able to update output images for video\n",
    "plt.close()\n",
    "%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "# disable axis display\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# input_path may be set to local file or URL \n",
    "input_path = \"/data/reference-sample-data/object-detection-python/cars_1900.mp4\"\n",
    "\n",
    "print (os.path.exists(input_path))\n",
    "\n",
    "print(\"Loading video [\",input_path,\"]\")\n",
    "# use OpenCV to load the input image\n",
    "cap = cv2.VideoCapture(input_path) \n",
    "\n",
    "# track frame count and set maximum\n",
    "frame_num = 0\n",
    "max_frame_num = 60\n",
    "skip_num_frames = 100\n",
    "last_frame_num = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "if last_frame_num < 1: last_frame_num = \"unknown\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    # read video frame\n",
    "    ret, image = cap.read()\n",
    "   \n",
    "    # break if no more video frames\n",
    "    if not ret:\n",
    "        break  \n",
    "    \n",
    "    frame_num += 1\n",
    "\n",
    "    # skip skip_num_frames of frames, then infer max_frame_num frames from there\n",
    "    if frame_num > skip_num_frames: \n",
    "        # infer image\n",
    "        inferImage(image, False)\n",
    "\n",
    "        # display results\n",
    "        # convert colors BGR -> RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # show image then force re-draw to show new image\n",
    "        plt.imshow(image)\n",
    "        plt.gcf().canvas.draw()\n",
    "    \n",
    "    # display current frame number\n",
    "    print(\"Frame #:\", frame_num, \"/\", last_frame_num, end=\"\\r\")\n",
    "    \n",
    "    # limit number of frames, video can be slow and gets slower the more frames that are processed\n",
    "    if frame_num >= (max_frame_num + skip_num_frames): \n",
    "        print(\"\\nStopping at frame #\", frame_num)\n",
    "        break\n",
    "\n",
    "print(\"\\nDone.\",frame_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #4: (Optional) Run your own video\n",
    "If you would like to see inference run on your own video, you may do so by first setting `input_path` to a local file or URL and then re-executing the cell above.  For example, you could use this video: https://github.com/intel-iot-devkit/sample-videos/raw/master/person-bicycle-car-detection.mp4 by replacing the `input_path=\"...\"` line above with the line:\n",
    "\n",
    "```python\n",
    "input_path=\"https://github.com/intel-iot-devkit/sample-videos/raw/master/person-bicycle-car-detection.mp4\"\n",
    "```\n",
    "\n",
    "You can control which frame to start from by setting `skip_num_frames` which will skip that many frames.\n",
    "You can also control how many frames to show by setting `max_frame_num`.\n",
    "<br><div class=note><i><b>Note: </b>There are more videos available to choose from at: https://github.com/intel-iot-devkit/sample-videos/</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Now that we are done running the sample, we clean up by deleting objects before exiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del exec_net\n",
    "del net\n",
    "del ie\n",
    "\n",
    "print(\"Resource objects removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- [More Jupyter Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials/) - additional sample application Jupyter* Notebook tutorials\n",
    "- [Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - sample applications\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2021.1)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "384px",
    "width": "200px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
