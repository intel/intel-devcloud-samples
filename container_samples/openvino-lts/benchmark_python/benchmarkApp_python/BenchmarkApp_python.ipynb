{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Benchmark Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO version check:\n",
    "You are currently using the Intel® Distribution of OpenVINO™ Toolkit LTS version. Alternatively, you can open a version of this notebook for the Intel® Distribution of OpenVINO™ Toolkit latest development version by [clicking this link](../../../openvino-dev-latest/tutorials/benchmarkApp_python/BenchmarkApp_python.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Benchmark Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the [OpenVINO benchmarking app](https://docs.openvinotoolkit.org/latest/_inference_engine_tools_benchmark_tool_README.html) to benchmark your model on different hardware.\n",
    "\n",
    "Benchmark python tool provides estimation of deep learning inference performance on the supported devices. Performance can be measured for two inference modes: synchronous (latency-oriented) and asynchronous (throughput-oriented).  \n",
    "\n",
    "This tutorial benchmarks the deep learning model with \n",
    "1. different hardware\n",
    "2. workload distribution with Multi plugin\n",
    "\n",
    "### How It Works\n",
    "Upon start-up, the application reads command-line parameters and loads a network and images/binary files to the Inference Engine plugin, which is chosen depending on a specified device. The number of infer requests and execution approach depend on the mode defined with the -api command-line parameter. \n",
    "\n",
    "In this tutorial, we use following input parameters with the benchmark app:\n",
    "\n",
    "- -m: deep learning model to infer in intermediate format, e.g. Resnet-50 tensorflow model. \n",
    "- -d: device to offload inference workload\n",
    "- -niter: number of iterations\n",
    "- -api: sync or async\n",
    "- --report_type: information about details counter e.g. FPS and latency\n",
    "- --report_folder: Path to a folder where statistics report is stored.\n",
    "- -i: input image/video, If a topology is not data sensitive, you can skip the input parameter. \n",
    "\n",
    "###  Setup the environment variables and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys                                                     \n",
    "from openvino.inference_engine import IECore\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from qarpo.model_visualizer_link import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install progress package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ./benchmark/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning model for inference\n",
    "This example uses a Tensorflow* implementation of a resnet-50 model for classification.\n",
    "\n",
    "#### Download the resnet-50 Tensorflow* model from the model downloader from the Intel distribution OpenVINO toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!downloader.py --name resnet-50-tf -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view a graph of the model used in this application, run the cell below then select the link generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showModelVisualizerLink(\"models/public/resnet-50-tf/resnet_v1-50.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize a deep-learning model using the Model Optimizer (MO) \n",
    "In this section, you will use the Model Optimizer to convert a trained model to two Intermediate Representation (IR) files (one .bin and one .xml). The Inference Engine requires this model conversion so that it can use the IR as input and achieve optimum performance on Intel® hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a directory to store IR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p models/FP32\n",
    "! mkdir -p models/FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the model with FP16 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mo.py \\\n",
    "--input_model models/public/resnet-50-tf/resnet_v1-50.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--mean_values=[123.68,116.78,103.94] \\\n",
    "-o models/FP16 \\\n",
    "--data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the model with FP32 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo.py \\\n",
    "--input_model models/public/resnet-50-tf/resnet_v1-50.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--mean_values=[123.68,116.78,103.94] \\\n",
    "-o models/FP32 \\\n",
    "--data_type FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating job file\n",
    "Till now, we ran all the above steps on a single edge system allocated for your account. Now we want to run the inference on different edge systems on the Intel IoT devcloud to benchmark the inference performance. For that, we will submit the inference jobs for each edge device in a queue. For each job, we will specify the type of the edge compute node that must be allocated for the job.\n",
    "\n",
    "The job file in the below cell is written in Bash, and will be executed directly on the edge compute node. Run the following cell to write this in to the file \"benchmark_app_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_app_job.sh\n",
    "\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "JOB_ID=`basename ${0} | cut -f1 -d\".\"`\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "#FP_MODEL=$3\n",
    "API=$3\n",
    "# Benchmark Application script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs - Updated for OpenVINO 2020.3\n",
    "    export AOCL_BOARD_PACKAGE_ROOT=/opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/BSP/a10_1150_sg2\n",
    "    source /opt/altera/aocl-pro-rte/aclrte-linux64/init_opencl.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_sg2_bitstreams/2020-3-2_PL2_FP16_InceptionV1_ResNet_YoloV3.aocx\n",
    "    export CL_CONTEXT_COMPILER_MODE_INTELFPGA=3\n",
    "fi\n",
    "\n",
    "if [ \"$DEVICE\" = \"HETERO:FPGA,CPU\" ] || [ \"$DEVICE\" = \"MYRIAD\" ] || [ \"$DEVICE\" = \"HDDL\" ] || [ \"$DEVICE\" = \"MULTI:HDDL,CPU\" ] || [ \"$DEVICE\" = \"MULTI:CPU,GPU\" ]; then\n",
    "    FP_MODEL=\"FP16\"\n",
    "else\n",
    "    FP_MODEL=\"FP32\"\n",
    "fi\n",
    "\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "\n",
    "mkdir -p ${OUTPUT_FILE}\n",
    "rm -f ${OUTPUT_FILE}/*\n",
    "\n",
    "echo ${SAMPLEPATH}/${OUTPUT_FILE} > benchmark_filename_${JOB_ID}.txt\n",
    "\n",
    "# Running the benchmark application code\n",
    "\n",
    "python3 benchmark_app.py -m ${SAMPLEPATH}/models/${FP_MODEL}/resnet_v1-50.xml \\\n",
    "            -d $DEVICE \\\n",
    "            -niter 10 \\\n",
    "            -api $API \\\n",
    "            --report_type detailed_counters \\\n",
    "            --report_folder ${SAMPLEPATH}/${OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the qsub command. We can submit object_detection_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of qsub command that we use for this:\n",
    "\n",
    "-l : this option lets us select the number and the type of nodes using nodes={node_count}:{property}.\n",
    "\n",
    "-F : this option lets us send arguments to the bash script.\n",
    "\n",
    "-N : this option lets use name the job so that it is easier to distinguish between them.\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait until the benchmarking report files are written \n",
    "\n",
    "We submit the job to different hardware platform using job queue. We will have to wait until we get the results back from our specified hardware. In the following script, we check if the reports file is generated that shows the job is complete. Until, the job is completed, we print dots on the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job_to_finish(job_id):\n",
    "    # import pandas as pd\n",
    "    \n",
    "    print(job_id[0]) \n",
    "    if job_id:\n",
    "        \n",
    "        print(\"Job submitted to the queue. Waiting for it to complete .\", end=\"\")\n",
    "        filename = \"benchmark_filename_{}.txt\".format(job_id[0].split(\".\")[0])\n",
    "        \n",
    "        while not os.path.exists(filename):  # Wait until the file report is created.\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\")\n",
    "        \n",
    "        # Print the results\n",
    "        with open(filename) as f:\n",
    "            results_dir = f.read().split(\"\\n\")[0]\n",
    "            \n",
    "        report_filename = os.path.join(results_dir, \"benchmark_report.csv\") # Wait until the file report is created.\n",
    "        while not os.path.exists(report_filename):\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\")\n",
    "        \n",
    "        df = pd.read_csv(report_filename, delimiter=\";\")\n",
    "        print(df)\n",
    "        \n",
    "        throughput = float(df.loc[\"throughput\"][0])\n",
    "        device = df.loc[\"target device\"][0]\n",
    "        load_time = float(df.loc[\"load network time (ms)\"][0])\n",
    "        read_time = float(df.loc[\"read network time (ms)\"][0])\n",
    "        \n",
    "        os.remove(filename) # Cleanup\n",
    "        \n",
    "    else:\n",
    "        print(\"Error in job submission.\")\n",
    "        \n",
    "        throughput = None\n",
    "        device = None\n",
    "        load_time = None\n",
    "        read_time = None\n",
    "        \n",
    "    return {\"Throughput (FPS)\": throughput, \n",
    "            \"Load network time (ms)\" : load_time,\n",
    "            \"Read network time (ms)\" : read_time}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above wait_for_job_to_finish() function returns throughput, load network time and read network time. We save these return values in a dictionary, benchamarks, to be used later in graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = {}  # Save the benchmarking results to a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Individual system with the deep learning model\n",
    "The Multi-Device plugin automatically assigns inference requests to available computational devices to execute the requests in parallel. \n",
    "\n",
    "This example shows how to use MULTI plugin from the Intel® Distribution of OpenVINO™ toolkit.\n",
    "First, let's take a look at the performance of each single Inference Engine device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run the Benchmark tool app with Intel® Core™ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub benchmark_app_job.sh -l nodes=1:idc001skl -F \"results/core CPU async\" \n",
    "benchmarks[\"Core\"] = wait_for_job_to_finish(job_id_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Run the Benchmark tool app with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub benchmark_app_job.sh -l nodes=1:idc007xv5 -F \"results/xeon/ CPU async\"      \n",
    "benchmarks[\"XeonE3\"] = wait_for_job_to_finish(job_id_xeon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run Benchmark tool application with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU and an Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub benchmark_app_job.sh -l nodes=1:idc001skl -F \"results/gpu GPU async\"        \n",
    "benchmarks[\"GPU\"] = wait_for_job_to_finish(job_id_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run Benchmark tool application with NCS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to an edge compute node with Intel NCS2...\")\n",
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub benchmark_app_job.sh -l nodes=1:idc004nc2 -F \"results/ncs2 MYRIAD async\"    \n",
    "benchmarks[\"NCS2\"] = wait_for_job_to_finish(job_id_ncs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Run Benchmark tool application with HDDL-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_hddlr = !qsub benchmark_app_job.sh -l nodes=1:idc002mx8 -F \"results/hddlr HDDL async\" \n",
    "benchmarks[\"HDDL-R\"] = wait_for_job_to_finish(job_id_hddlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Run Benchmark tool application with IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core™ i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub benchmark_app_job.sh -l nodes=1:idc003a10 -F \"results/fpga HETERO:FPGA,CPU async\" \n",
    "benchmarks[\"HETERO:FPGA,CPU\"] = wait_for_job_to_finish(job_id_fpga) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi plugin\n",
    "Now let's try [MULTI plugin](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_MULTI.html) with different combination of available Inference Engine devices.\n",
    "\n",
    "Multi-Device plugin automatically assigns inference requests to available computational devices to execute the requests in parallel. Potential gains are as follows:\n",
    "\n",
    "- Improved throughput that multiple devices can deliver (compared to single-device execution)\n",
    "- More consistent performance, since the devices can now share the inference burden (so that if one device is becoming too busy, another device can take more of the load)\n",
    "\n",
    "Notice that with multi-device the application logic left unchanged, so you don't need to explicitly load the network to every device, create and balance the inference requests and so on.\n",
    "\n",
    "#### 1. Run Benchmark tool application with MULTI:CPU,GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an CPU and GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_cpu_gpu = !qsub benchmark_app_job.sh -l nodes=1:idc001skl -F \"results/cpu_gpu MULTI:CPU,GPU sync\" \n",
    "benchmarks[\"MULTI:CPU,GPU\"] = wait_for_job_to_finish(job_id_cpu_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run Benchmark tool application with MULTI:CPU,HDDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with Intel CPU and Intel Movidius HDDL-R...\")\n",
    "#Submit job to the queue\n",
    "job_id_cpu_hddl = !qsub benchmark_app_job.sh -l nodes=1:idc002mx8 -F \"results/cpu_hddl MULTI:HDDL,CPU async\" \n",
    "benchmarks[\"MULTI:HDDL,CPU\"] = wait_for_job_to_finish(job_id_cpu_hddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in benchmark{} dictionary. Run the cell below to plot the results of all jobs side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the benchmarking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_benchmarks(metric):\n",
    "    \n",
    "    latency = {}\n",
    "    no_number = False\n",
    "    for device in benchmarks:\n",
    "        if isinstance(benchmarks[device][metric], str):\n",
    "            no_number = True\n",
    "        else:\n",
    "            latency[device] = benchmarks[device][metric]    \n",
    "    \n",
    "    if not no_number:\n",
    "        plt.figure(figsize=(18,8))\n",
    "        plt.bar(*zip(*latency.items()));\n",
    "        plt.xticks(fontsize=14);\n",
    "        plt.yticks(fontsize=18);\n",
    "        plt.ylabel(metric, fontsize=20);\n",
    "\n",
    "        rects = plt.gca().patches\n",
    "\n",
    "        # Make some labels.\n",
    "        labels = [\"{:,.2f}\".format(i) for i in latency.values()]\n",
    "\n",
    "        for rect, label in zip(rects, labels):\n",
    "            height = rect.get_height()\n",
    "            plt.gca().text(rect.get_x() + rect.get_width() / 2, height/2.0, label,\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=20, color=\"white\", path_effects=[PathEffects.withStroke(linewidth=2, foreground=\"black\")])\n",
    "            \n",
    "    else:\n",
    "        print(\"ERROR: Field '{}' has text strings. Can't plot it.\".format(metric))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Read network time - It is the time to read the deep learning model from its stored location. \n",
    "\n",
    "Load network time - It is the time to load the deep learning model to the device plugin where the inference should happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_benchmarks, metric=benchmarks[next(iter(benchmarks))].keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- [More Jupyter* Notebook Samples](https://devcloud.intel.com/edge/advanced/sample_applications/) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://devcloud.intel.com/edge/get_started/tutorials) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (OpenVINO 2020.3.2 LTS)",
   "language": "python",
   "name": "c003-python_3_lts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
