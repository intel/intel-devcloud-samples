{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Benchmark Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Benchmark Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample provides a comparison of performance benchmarks using the [OpenVINO Benchmarking app](https://docs.openvinotoolkit.org/latest/_inference_engine_tools_benchmark_tool_README.html). The Benchmark python tool provides estimation of deep learning inference performance on the supported devices. \n",
    "\n",
    "Upon start-up, the application reads command-line parameters and loads a network and images/binary files to the Inference Engine plugin, which is chosen depending on a specified device. \n",
    "\n",
    "###  Setup the environment variables and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys                                                     \n",
    "from openvino.inference_engine import IECore\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from qarpo.model_visualizer_link import *\n",
    "from qarpo.demoutils import liveQstat\n",
    "from job_utils import create_jobs_from_config, wait_for_job_to_finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install progress package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ./benchmark/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning model for inference\n",
    "This example uses a Tensorflow* implementation of a resnet-50 model for classification.\n",
    "\n",
    "#### Download the resnet-50 Tensorflow* model from the model downloader from the Intel distribution OpenVINO toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_downloader --name resnet-50-tf -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view a graph of the model used in this application, run the cell below then select the link generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showModelVisualizerLink(\"models/public/resnet-50-tf/resnet_v1-50.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize a deep-learning model using the Model Optimizer (MO) \n",
    "In this section, you will use the Model Optimizer to convert a trained model to two Intermediate Representation (IR) files (one .bin and one .xml). The Inference Engine requires this model conversion so that it can use the IR as input and achieve optimum performance on Intel® hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a directory to store IR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p models/FP32\n",
    "! mkdir -p models/FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the model with FP16 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mo \\\n",
    "--input_model models/public/resnet-50-tf/resnet_v1-50.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--mean_values=[123.68,116.78,103.94] \\\n",
    "-o models/FP16 \\\n",
    "--data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the model with FP32 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo \\\n",
    "--input_model models/public/resnet-50-tf/resnet_v1-50.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--mean_values=[123.68,116.78,103.94] \\\n",
    "-o models/FP32 \\\n",
    "--data_type FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating job file\n",
    "Till now, we ran all the above steps on a single edge system allocated for your account. Now we want to run the inference on different edge systems on the Intel IoT devcloud to benchmark the inference performance. For that, we will submit the inference jobs for each edge device in a queue. For each job, we will specify the type of the edge compute node that must be allocated for the job.\n",
    "\n",
    "The job file in the below cell is written in Bash, and will be executed directly on the edge compute node. Run the following cell to write this in to the file \"benchmark_app_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_app_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "JOB_ID=`basename ${0} | cut -f1 -d\".\"`\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "API=$3\n",
    "\n",
    "echo VENV_PATH=$VENV_PATH\n",
    "echo OPENVINO_RUNTIME=$OPENVINO_RUNTIME\n",
    "\n",
    "# Follow this order of setting up environment for openVINO 2022.1.0.553\n",
    "echo \"Activating a Python virtual environment from ${VENV_PATH}...\"\n",
    "source ${VENV_PATH}/bin/activate\n",
    "echo \"Activating OpenVINO variables from ${OPENVINO_RUNTIME}...\"\n",
    "source ${OPENVINO_RUNTIME}/setupvars.sh\n",
    "\n",
    "# Benchmark Application script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "\n",
    "mkdir -p ${OUTPUT_FILE}\n",
    "rm -f ${OUTPUT_FILE}/*\n",
    "\n",
    "FP_MODEL=\"FP16\"\n",
    "\n",
    "# Running the benchmark application code\n",
    "\n",
    "python3 benchmark_app.py -m ${SAMPLEPATH}/models/${FP_MODEL}/resnet_v1-50.xml \\\n",
    "            -d $DEVICE \\\n",
    "            -niter 10 \\\n",
    "            -api $API \\\n",
    "            --report_type detailed_counters \\\n",
    "            --report_folder ${SAMPLEPATH}/${OUTPUT_FILE}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark systems with the deep learning model\n",
    "The following code block creates job execution commands for the systems listed in the job_config.json file. The jobs will be queued and then run when an instance of the hardware is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import json\n",
    "import qarpo\n",
    "\n",
    "# load job configurations for demo\n",
    "with open('job_config.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    \n",
    "# create and run the user job interface\n",
    "job_interface = qarpo.Interface(data)\n",
    "job_interface.displayUI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor job status\n",
    "\n",
    "To check the status of the jobs that have been submitted, use the `qstat` command.  The custom Jupyter* Notebook widget `liveQstat()` is provided to display the output of `qstat` with live updates.  \n",
    "\n",
    "Run the following cell to display the current job status with periodic updates. \n",
    "You should see the jobs that you have submitted (referenced by the `JobID` that gets displayed right after you submit the jobs in the previous step).\n",
    "There should also be an extra job in the queue named `jupyterhub-singleuser`: this job is your current Jupyter* Notebook session which is always running.\n",
    "\n",
    "The `S` column shows the current status of each job: \n",
    "- If the status is `Q`, then the job is queued and waiting for available resources\n",
    "- If the status is `R`, then the job is running\n",
    "- If the job is no longer listed, then the job has completed\n",
    "\n",
    "<br><div class=note><i><b>\n",
    "Note: The amount of time spent in the queue depends on the number of users accessing the requested compute nodes. Once the jobs for this sample application begin to run, they should take from 1 to 5 minutes each to complete.\n",
    "</b></i></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- [More Jupyter* Notebook Samples](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge/build/sample-apps.html) - additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge/learn/tutorials.html) - sample application Jupyter* Notebook tutorials\n",
    "- [Intel® Distribution of OpenVINO™ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the Intel® Distribution of OpenVINO™ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# About this notebook\n",
    "\n",
    "For technical support, please see the [Intel® DevCloud Forums](https://community.intel.com/t5/Intel-DevCloud/bd-p/devcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>Intel® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (OpenVINO 2022.2.0)",
   "language": "python",
   "name": "openvino_2022.2-python36-prod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
