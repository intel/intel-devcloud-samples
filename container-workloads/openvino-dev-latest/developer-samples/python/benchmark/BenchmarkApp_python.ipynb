{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Benchmark Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Benchmark Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the [OpenVINO benchmarking app](https://docs.openvinotoolkit.org/latest/_inference_engine_tools_benchmark_tool_README.html) to benchmark your model on different hardware.\n",
    "\n",
    "Benchmark python tool provides estimation of deep learning inference performance on the supported devices. Performance can be measured for two inference modes: synchronous (latency-oriented) and asynchronous (throughput-oriented).  \n",
    "\n",
    "This tutorial benchmarks the deep learning model with \n",
    "1. different hardware\n",
    "2. workload distribution with Multi plugin\n",
    "\n",
    "### How It Works\n",
    "Upon start-up, the application reads command-line parameters and loads a network and images/binary files to the Inference Engine plugin, which is chosen depending on a specified device. The number of infer requests and execution approach depend on the mode defined with the -api command-line parameter. \n",
    "\n",
    "In this tutorial, we use following input parameters with the benchmark app:\n",
    "\n",
    "- -m: deep learning model to infer in intermediate format, e.g. Resnet-50 tensorflow model. \n",
    "- -d: device to offload inference workload\n",
    "- -niter: number of iterations\n",
    "- -api: sync or async\n",
    "- --report_type: information about details counter e.g. FPS and latency\n",
    "- --report_folder: Path to a folder where statistics report is stored.\n",
    "- -i: input image/video, If a topology is not data sensitive, you can skip the input parameter. \n",
    "\n",
    "###  Setup the environment variables and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys                                                     \n",
    "from openvino.inference_engine import IECore\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from qarpo.model_visualizer_link import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install progress package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r ./benchmark/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning model for inference\n",
    "This example uses a Tensorflow* implementation of a resnet-50 model for classification.\n",
    "\n",
    "#### Download the resnet-50 Tensorflow* model from the model downloader from the Intel distribution OpenVINO toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_downloader --name resnet-50-tf -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view a graph of the model used in this application, run the cell below then select the link generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showModelVisualizerLink(\"models/public/resnet-50-tf/resnet_v1-50.pb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize a deep-learning model using the Model Optimizer (MO) \n",
    "In this section, you will use the Model Optimizer to convert a trained model to two Intermediate Representation (IR) files (one .bin and one .xml). The Inference Engine requires this model conversion so that it can use the IR as input and achieve optimum performance on IntelÂ® hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a directory to store IR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p models/FP32\n",
    "! mkdir -p models/FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the model with FP16 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mo \\\n",
    "--input_model models/public/resnet-50-tf/resnet_v1-50.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--mean_values=[123.68,116.78,103.94] \\\n",
    "-o models/FP16 \\\n",
    "--data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert the model with FP32 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo \\\n",
    "--input_model models/public/resnet-50-tf/resnet_v1-50.pb \\\n",
    "--input_shape=[1,224,224,3] \\\n",
    "--mean_values=[123.68,116.78,103.94] \\\n",
    "-o models/FP32 \\\n",
    "--data_type FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating job file\n",
    "Till now, we ran all the above steps on a single edge system allocated for your account. Now we want to run the inference on different edge systems on the Intel IoT devcloud to benchmark the inference performance. For that, we will submit the inference jobs for each edge device in a queue. For each job, we will specify the type of the edge compute node that must be allocated for the job.\n",
    "\n",
    "The job file in the below cell is written in Bash, and will be executed directly on the edge compute node. Run the following cell to write this in to the file \"benchmark_app_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_app_job.sh\n",
    "\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "JOB_ID=`basename ${0} | cut -f1 -d\".\"`\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "#FP_MODEL=$3\n",
    "API=$3\n",
    "# Benchmark Application script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "\n",
    "if  [ \"$DEVICE\" = \"MYRIAD\" ] || [ \"$DEVICE\" = \"HDDL\" ] || [ \"$DEVICE\" = \"MULTI:HDDL,CPU\" ] || [ \"$DEVICE\" = \"MULTI:CPU,GPU\" ]; then\n",
    "    FP_MODEL=\"FP16\"\n",
    "else\n",
    "    FP_MODEL=\"FP32\"\n",
    "fi\n",
    "echo VENV_PATH=$VENV_PATH\n",
    "echo OPENVINO_RUNTIME=$OPENVINO_RUNTIME\n",
    "echo INPUT_FILE=$INPUT_FILE\n",
    "echo FP_MODEL=$FP_MODEL\n",
    "echo INPUT_TILE=$INPUT_FILE\n",
    "echo NUM_REQS=$NUM_REQS\n",
    "\n",
    "# Follow this order of setting up environment for openVINO 2022.1.0.553\n",
    "echo \"Activating a Python virtual environment from ${VENV_PATH}...\"\n",
    "source ${VENV_PATH}/bin/activate\n",
    "echo \"Activating OpenVINO variables from ${OPENVINO_RUNTIME}...\"\n",
    "source ${OPENVINO_RUNTIME}/setupvars.sh\n",
    "\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "\n",
    "mkdir -p ${OUTPUT_FILE}\n",
    "rm -f ${OUTPUT_FILE}/*\n",
    "\n",
    "echo ${SAMPLEPATH}/${OUTPUT_FILE} > benchmark_filename_${JOB_ID}.txt\n",
    "\n",
    "# Running the benchmark application code\n",
    "\n",
    "python3 benchmark_app.py -m ${SAMPLEPATH}/models/${FP_MODEL}/resnet_v1-50.xml \\\n",
    "            -d $DEVICE \\\n",
    "            -niter 10 \\\n",
    "            -api $API \\\n",
    "            --report_type detailed_counters \\\n",
    "            --report_folder ${SAMPLEPATH}/${OUTPUT_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the qsub command. We can submit object_detection_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of qsub command that we use for this:\n",
    "\n",
    "-l : this option lets us select the number and the type of nodes using nodes={node_count}:{property}.\n",
    "\n",
    "-F : this option lets us send arguments to the bash script.\n",
    "\n",
    "-N : this option lets use name the job so that it is easier to distinguish between them.\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait until the benchmarking report files are written \n",
    "\n",
    "We submit the job to different hardware platform using job queue. We will have to wait until we get the results back from our specified hardware. In the following script, we check if the reports file is generated that shows the job is complete. Until, the job is completed, we print dots on the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job_to_finish(job_id):\n",
    "    # import pandas as pd\n",
    "    \n",
    "    print(job_id[0]) \n",
    "    if job_id:\n",
    "        \n",
    "        print(\"Job submitted to the queue. Waiting for it to complete .\", end=\"\")\n",
    "        filename = \"benchmark_filename_{}.txt\".format(job_id[0].split(\".\")[0])\n",
    "        \n",
    "        while not os.path.exists(filename):  # Wait until the file report is created.\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\")\n",
    "        \n",
    "        # Print the results\n",
    "        with open(filename) as f:\n",
    "            results_dir = f.read().split(\"\\n\")[0]\n",
    "            \n",
    "        report_filename = os.path.join(results_dir, \"benchmark_report.csv\") # Wait until the file report is created.\n",
    "        while not os.path.exists(report_filename):\n",
    "            time.sleep(1)\n",
    "            print(\".\", end=\"\")\n",
    "        \n",
    "        df = pd.read_csv(report_filename, delimiter=\";\")\n",
    "        print(df)\n",
    "        \n",
    "        throughput = float(df.loc[\"throughput\"][0])\n",
    "        device = df.loc[\"target device\"][0]\n",
    "        load_time = float(df.loc[\"load network time (ms)\"][0])\n",
    "        read_time = float(df.loc[\"read network time (ms)\"][0])\n",
    "        \n",
    "        os.remove(filename) # Cleanup\n",
    "        \n",
    "    else:\n",
    "        print(\"Error in job submission.\")\n",
    "        \n",
    "        throughput = None\n",
    "        device = None\n",
    "        load_time = None\n",
    "        read_time = None\n",
    "        \n",
    "    return {\"Throughput (FPS)\": throughput, \n",
    "            \"Load network time (ms)\" : load_time,\n",
    "            \"Read network time (ms)\" : read_time}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above wait_for_job_to_finish() function returns throughput, load network time and read network time. We save these return values in a dictionary, benchamarks, to be used later in graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = {}  # Save the benchmarking results to a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Individual system with the deep learning model\n",
    "The Multi-Device plugin automatically assigns inference requests to available computational devices to execute the requests in parallel. \n",
    "\n",
    "This example shows how to use MULTI plugin from the IntelÂ® Distribution of OpenVINOâ¢ toolkit.\n",
    "First, let's take a look at the performance of each single Inference Engine device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Run the Benchmark tool app with IntelÂ® Coreâ¢ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub benchmark_app_job.sh -l nodes=1:idc001skl -F \"results/core CPU async\" -v VENV_PATH,OPENVINO_RUNTIME\n",
    "benchmarks[\"Core\"] = wait_for_job_to_finish(job_id_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Run the Benchmark tool app with IntelÂ® XeonÂ® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">IntelÂ® \n",
    "    XeonÂ® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub benchmark_app_job.sh -l nodes=1:idc007xv5 -F \"results/xeon/ CPU async\"  -v VENV_PATH,OPENVINO_RUNTIME\n",
    "benchmarks[\"XeonE3\"] = wait_for_job_to_finish(job_id_xeon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run Benchmark tool application with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU and an Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub benchmark_app_job.sh -l nodes=1:idc001skl -F \"results/gpu GPU async\"    -v VENV_PATH,OPENVINO_RUNTIME   \n",
    "benchmarks[\"GPU\"] = wait_for_job_to_finish(job_id_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run Benchmark tool application with NCS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Submitting job to an edge compute node with Intel NCS2...\")\n",
    "#Submit job to the queue\n",
    "job_id_ncs2 = !qsub benchmark_app_job.sh -l nodes=1:idc004nc2 -F \"results/ncs2 MYRIAD async\"    -v VENV_PATH,OPENVINO_RUNTIME\n",
    "benchmarks[\"NCS2\"] = wait_for_job_to_finish(job_id_ncs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Run Benchmark tool application with HDDL-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_hddlr = !qsub benchmark_app_job.sh -l nodes=1:idc002mx8 -F \"results/hddlr HDDL async\" -v VENV_PATH,OPENVINO_RUNTIME\n",
    "benchmarks[\"HDDL-R\"] = wait_for_job_to_finish(job_id_hddlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi plugin\n",
    "Now let's try [MULTI plugin](https://docs.openvinotoolkit.org/latest/_docs_IE_DG_supported_plugins_MULTI.html) with different combination of available Inference Engine devices.\n",
    "\n",
    "Multi-Device plugin automatically assigns inference requests to available computational devices to execute the requests in parallel. Potential gains are as follows:\n",
    "\n",
    "- Improved throughput that multiple devices can deliver (compared to single-device execution)\n",
    "- More consistent performance, since the devices can now share the inference burden (so that if one device is becoming too busy, another device can take more of the load)\n",
    "\n",
    "Notice that with multi-device the application logic left unchanged, so you don't need to explicitly load the network to every device, create and balance the inference requests and so on.\n",
    "\n",
    "#### 1. Run Benchmark tool application with MULTI:CPU,GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an CPU and GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_cpu_gpu = !qsub benchmark_app_job.sh -l nodes=1:idc001skl -F \"results/cpu_gpu MULTI:CPU,GPU sync\" -v VENV_PATH,OPENVINO_RUNTIME\n",
    "benchmarks[\"MULTI:CPU,GPU\"] = wait_for_job_to_finish(job_id_cpu_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run Benchmark tool application with MULTI:CPU,HDDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with Intel CPU and Intel Movidius HDDL-R...\")\n",
    "#Submit job to the queue\n",
    "job_id_cpu_hddl = !qsub benchmark_app_job.sh -l nodes=1:idc002mx8 -F \"results/cpu_hddl MULTI:HDDL,CPU async\" -v VENV_PATH,OPENVINO_RUNTIME\n",
    "benchmarks[\"MULTI:HDDL,CPU\"] = wait_for_job_to_finish(job_id_cpu_hddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in benchmark{} dictionary. Run the cell below to plot the results of all jobs side-by-side.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the benchmarking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_benchmarks(metric):\n",
    "    \n",
    "    latency = {}\n",
    "    no_number = False\n",
    "    for device in benchmarks:\n",
    "        if isinstance(benchmarks[device][metric], str):\n",
    "            no_number = True\n",
    "        else:\n",
    "            latency[device] = benchmarks[device][metric]    \n",
    "    \n",
    "    if not no_number:\n",
    "        plt.figure(figsize=(18,8))\n",
    "        plt.bar(*zip(*latency.items()));\n",
    "        plt.xticks(fontsize=14);\n",
    "        plt.yticks(fontsize=18);\n",
    "        plt.ylabel(metric, fontsize=20);\n",
    "\n",
    "        rects = plt.gca().patches\n",
    "\n",
    "        # Make some labels.\n",
    "        labels = [\"{:,.2f}\".format(i) for i in latency.values()]\n",
    "\n",
    "        for rect, label in zip(rects, labels):\n",
    "            height = rect.get_height()\n",
    "            plt.gca().text(rect.get_x() + rect.get_width() / 2, height/2.0, label,\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=20, color=\"white\", path_effects=[PathEffects.withStroke(linewidth=2, foreground=\"black\")])\n",
    "            \n",
    "    else:\n",
    "        print(\"ERROR: Field '{}' has text strings. Can't plot it.\".format(metric))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "Read network time - It is the time to read the deep learning model from its stored location. \n",
    "\n",
    "Load network time - It is the time to load the deep learning model to the device plugin where the inference should happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_benchmarks, metric=benchmarks[next(iter(benchmarks))].keys());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- [More Jupyter* Notebook Samples](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge/build/sample-apps.html)- additional sample applications \n",
    "- [Jupyter* Notebook Tutorials](https://software.intel.com/content/www/us/en/develop/tools/devcloud/edge/learn/tutorials.html) - sample application Jupyter* Notebook tutorials\n",
    "- [IntelÂ® Distribution of OpenVINOâ¢ toolkit Main Page](https://software.intel.com/openvino-toolkit) - learn more about the tools and use of the IntelÂ® Distribution of OpenVINOâ¢ toolkit for implementing inference on the edge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this notebook\n",
    "\n",
    "For technical support, please see the [IntelÂ® DevCloud Forums](https://software.intel.com/en-us/forums/intel-devcloud-for-edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=background-color:#0071C5;color:white;padding:0.5em;display:table-cell;width:100pc;vertical-align:middle>\n",
    "<img style=float:right src=\"https://devcloud.intel.com/edge/static/images/svg/IDZ_logo.svg\" alt=\"Intel DevCloud logo\" width=\"150px\"/>\n",
    "<a style=color:white>IntelÂ® DevCloud for the Edge</a><br>   \n",
    "<a style=color:white href=\"#top\">Top of Page</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Intel-DevCloud-for-the-Edge-Usage-Agreement.pdf\">Usage Agreement (Intel)</a> | \n",
    "<a style=color:white href=\"https://devcloud.intel.com/edge/static/docs/terms/Colfax_Cloud_Service_Terms_v1.3.pdf\">Service Terms (Colfax)</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenVINO 2022.1.0",
   "language": "python",
   "name": "openvino_2022.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "613.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
